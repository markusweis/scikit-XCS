{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scikit-XCS Complete User Guide\n",
    "\n",
    "Author: Robert Zhang - University of Pennsylvania, B.S.E Computer Science, B.S.E. Economics (SEAS '22, WH '22)\n",
    "\n",
    "Advisor: Ryan Urbanowicz, PhD - University of Pennsylvania, Department of Biostatics, Epidemiology, and Informatics & Institue for Biomedical Informatics (IBI)\n",
    "\n",
    "Special thanks to Will N. Browne (Victoria University of Wellingston), for his valuable input in the compilation of the scikit-XCS algorithm\n",
    "\n",
    "Date: 04/30/2020\n",
    "\n",
    "Package Requirements: (Python 3)\n",
    "<ul>\n",
    "    <li>pandas</li>\n",
    "    <li>numpy</li>\n",
    "    <li>scikit-learn</li>\n",
    "</ul>\n",
    "\n",
    "Notebook Requirements:\n",
    "<ul>\n",
    "    <li>scikit-XCS</li>\n",
    "    <li>scikit-learn</li>\n",
    "    <li>pandas</li>\n",
    "    <li>mathplotlib</li>\n",
    "</ul>\n",
    "\n",
    "## Introduction\n",
    "This notebook presents a complete user guide to the core sklearn-compatible scikit-XCS package as well as auxilliary helper objects included in the package. XCS is the most popular and best studied learning classifier system algorithm to date. In general, Learning Classifier Systems (LCSs) are a classification of Rule Based Machine Learning Algorithms that have been shown to perform well on problems involving high amounts of heterogeneity and epistasis. Well designed LCSs are also highly human interpretable. LCS variants have been shown to adeptly handle supervised and reinforced, classification and regression, online and offline learning problems, as well as missing or unbalanced data. These characteristics of versatility and interpretability give LCSs a wide range of potential applications, notably those in biomedicine. This package is still under active development and we encourage you to check back on this repository for updates.\n",
    "\n",
    "This version of scikit-XCS is suitable for **single step, classification** problems. It has not yet been developed for multi-step reinforcement learning problems nor regression problems. Within these bounds however, scikit-XCS can be applied to almost any supervised classification data set and supports:\n",
    "\n",
    "<ul>\n",
    "    <li>Feature sets that are discrete/categorical, continuous-valued or a mix of both</li>\n",
    "    <li>Data with missing values</li>\n",
    "    <li>Binary Classification Problems (Binary Endpoints)</li>\n",
    "    <li>Multi-class Classification Problems (Multi-class Endpoints)</li>\n",
    "</ul>\n",
    "\n",
    "## Notebook Organization\n",
    "\n",
    "**Part 1: Loading Data**\n",
    "<ul>\n",
    "    <li> Dataset format requirements for scikit-XCS</li>\n",
    "    <li> Method 1: Loading data using pandas</li>\n",
    "    <li> Method 2: Loading data using StringEnumerator (additional data loading/transformation tool provided by scikit-XCS package)</li>\n",
    "</ul>\n",
    "\n",
    "**Part 2: Initializing XCS Estimator Object**\n",
    "<ul>\n",
    "    <li> Basic Initialization</li>\n",
    "    <li> Overview of all scikit-XCS initialization parameters and default values</li>\n",
    "</ul>\n",
    "\n",
    "**Part 3: Model Training and Testing**\n",
    "<ul>\n",
    "    <li> model.fit(X,y) and how it works</li>\n",
    "    <li> model.predict(X), model.predict_proba(X)</li>\n",
    "    <li> model Cross Validation and Scoring</li>\n",
    "    <li> ROC/PRC curves and AUC</li>\n",
    "    <li> Auxilliary scikit-XCS methods\n",
    "</ul>\n",
    "\n",
    "**Part 4: Data Collection and Export**\n",
    "<ul>\n",
    "    <li> Exporting and Accessing Iteration Tracking Data </li>\n",
    "    <li> Exporting and Accessing Rule Population Data </li>\n",
    "</ul>\n",
    "\n",
    "**Part 5: Population Reboot**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Loading Data\n",
    "\n",
    "### Dataset Format Requirement for scikit-XCS\n",
    "\n",
    "There are 4 requirements for the dataset:\n",
    "<ul>\n",
    "    <li>Data Attributes for all data instances are represented by an <b>nxm</b> numpy.ndarray, where <b>n</b> = # of instances and <b>m</b> = # of attributes for each instance</li>\n",
    "    <li>Data Actions for all data instances are represented by a 1 dimensional numpy.ndarray of length <b>n</b></li>\n",
    "    <li>Both the <b>nxm</b> attribute array and the length <b>n</b> action array are fully numeric. This means that every element in each array must be convertable to a float. String, boolean types are not permitted (of course, boolean datasets can be represented by binary 0s and 1s). It is important to mention that missing data in the attribute dataset is acceptable, if represented as a NaN type within the array. Missing data in the action array is not acceptable.</li>\n",
    "    <li>As mentioned aboved, the problem must be a classification problem. This version does not support regression problems.</li>\n",
    "</ul>\n",
    "\n",
    "The package includes automated parameter checking that will raise an Exception at the start of training if the dataset does not meet the above requirements.\n",
    "\n",
    "Below, we present 2 methods to load data of the correct format for the XCS model. The first is the commonly used pandas. The second is a custom, standalone method included with this package named StringEnumerator, that will make your life much easier in terms of ensuring data formatting requirements are met for the XCS model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data Method 1: Using pandas\n",
    "\n",
    "There are many ways you can derive the attribute and phenotype array. One most common method is by using pandas via reading from a csv file, as the below code demonstrates with the a 6 bit multiplexer dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Features\n",
      "[[0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1]\n",
      " [0 0 0 0 1 0]\n",
      " [0 0 0 0 1 1]\n",
      " [0 0 0 1 0 0]\n",
      " [0 0 0 1 0 1]\n",
      " [0 0 0 1 1 0]\n",
      " [0 0 0 1 1 1]\n",
      " [0 0 1 0 0 0]\n",
      " [0 0 1 0 0 1]\n",
      " [0 0 1 0 1 0]\n",
      " [0 0 1 0 1 1]\n",
      " [0 0 1 1 0 0]\n",
      " [0 0 1 1 0 1]\n",
      " [0 0 1 1 1 0]\n",
      " [0 0 1 1 1 1]\n",
      " [0 1 0 0 0 0]\n",
      " [0 1 0 0 0 1]\n",
      " [0 1 0 0 1 0]\n",
      " [0 1 0 0 1 1]\n",
      " [0 1 0 1 0 0]\n",
      " [0 1 0 1 0 1]\n",
      " [0 1 0 1 1 0]\n",
      " [0 1 0 1 1 1]\n",
      " [0 1 1 0 0 0]\n",
      " [0 1 1 0 0 1]\n",
      " [0 1 1 0 1 0]\n",
      " [0 1 1 0 1 1]\n",
      " [0 1 1 1 0 0]\n",
      " [0 1 1 1 0 1]\n",
      " [0 1 1 1 1 0]\n",
      " [0 1 1 1 1 1]\n",
      " [1 0 0 0 0 0]\n",
      " [1 0 0 0 0 1]\n",
      " [1 0 0 0 1 0]\n",
      " [1 0 0 0 1 1]\n",
      " [1 0 0 1 0 0]\n",
      " [1 0 0 1 0 1]\n",
      " [1 0 0 1 1 0]\n",
      " [1 0 0 1 1 1]\n",
      " [1 0 1 0 0 0]\n",
      " [1 0 1 0 0 1]\n",
      " [1 0 1 0 1 0]\n",
      " [1 0 1 0 1 1]\n",
      " [1 0 1 1 0 0]\n",
      " [1 0 1 1 0 1]\n",
      " [1 0 1 1 1 0]\n",
      " [1 0 1 1 1 1]\n",
      " [1 1 0 0 0 0]\n",
      " [1 1 0 0 0 1]\n",
      " [1 1 0 0 1 0]\n",
      " [1 1 0 0 1 1]\n",
      " [1 1 0 1 0 0]\n",
      " [1 1 0 1 0 1]\n",
      " [1 1 0 1 1 0]\n",
      " [1 1 0 1 1 1]\n",
      " [1 1 1 0 0 0]\n",
      " [1 1 1 0 0 1]\n",
      " [1 1 1 0 1 0]\n",
      " [1 1 1 0 1 1]\n",
      " [1 1 1 1 0 0]\n",
      " [1 1 1 1 0 1]\n",
      " [1 1 1 1 1 0]\n",
      " [1 1 1 1 1 1]]\n",
      "\n",
      "Data Actions\n",
      "[0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 1 1 0\n",
      " 0 1 1 0 0 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1]\n",
      "\n",
      "Data Headers\n",
      "['A_0' 'A_1' 'R_0' 'R_1' 'R_2' 'R_3']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Read from CSV file\n",
    "data = pd.read_csv(\"test/DataSets/Real/Multiplexer6.csv\")\n",
    "\n",
    "#Specify the dataset's action label\n",
    "actionLabel = \"class\"\n",
    "\n",
    "#Derive the attribute and action array using the action label\n",
    "dataFeatures = data.drop(actionLabel,axis = 1).values\n",
    "dataActions = data[actionLabel].values\n",
    "\n",
    "#Optional: Retrieve the headers for each attribute as a length n array\n",
    "dataHeaders = data.drop(actionLabel,axis=1).columns.values\n",
    "\n",
    "print(\"Data Features\")\n",
    "print(dataFeatures)\n",
    "print(\"\\nData Actions\")\n",
    "print(dataActions)\n",
    "print(\"\\nData Headers\")\n",
    "print(dataHeaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is another code snippet that uses the same method, but with a dataset that contains missing values. Note that all missing values must be represented as a NaN type when used in the scikit-XCS model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Features\n",
      "[[ 1. nan  1.  4.]\n",
      " [ 2.  0.  1. nan]\n",
      " [ 4. nan  1.  2.]\n",
      " [nan  1. nan  1.]\n",
      " [ 6. nan  1.  1.]]\n",
      "\n",
      "Data Actions\n",
      "[1 0 1 0 1]\n",
      "\n",
      "Data Headers\n",
      "['N1' 'N2' 'N3' 'N4']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Read from CSV file\n",
    "data = pd.read_csv(\"test/DataSets/Tests/MissingFeatureData.csv\")\n",
    "\n",
    "#Specify the dataset's action label\n",
    "actionLabel = \"phenotype\"\n",
    "\n",
    "#Derive the attribute and action array using the action label\n",
    "dataFeatures = data.drop(actionLabel,axis = 1).values\n",
    "dataActions = data[actionLabel].values\n",
    "\n",
    "#Optional: Retrieve the headers for each attribute as a length n array\n",
    "dataHeaders = data.drop(actionLabel,axis=1).columns.values\n",
    "\n",
    "print(\"Data Features\")\n",
    "print(dataFeatures)\n",
    "print(\"\\nData Actions\")\n",
    "print(dataActions)\n",
    "print(\"\\nData Headers\")\n",
    "print(dataHeaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data Method 2: Using StringEnumerator\n",
    "\n",
    "Given the 4 requirements for the attribute and phenotype array, not all datasets necessarily satisfy those requirements. For example, a dataset may contain an attribute with values that are colors \"red\", \"green\", \"blue\". Since the scikit-XCS model can only work with numeric data, this dataset would not meet the requirements. Alternatively, a dataset may, for some reason, have missing phenotype data. Since the scikit-XCS model requires that the phenotype array is complete, this dataset would also not meet the requirements.\n",
    "\n",
    "To make things more convenient, included in this package is an auxilliary data type called **String Enumerator** that makes it easy to transform datasets that do not meet the requirements into datasets that do.\n",
    "\n",
    "For example, take the below dummy dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Features\n",
      "[['male' 1.2 'young']\n",
      " ['female' 0.3 nan]\n",
      " ['female' -0.4 'old']\n",
      " [nan 0.0 'young']]\n",
      "\n",
      "Data Actions\n",
      "['china' nan 'china' 'russia']\n",
      "\n",
      "Data Headers\n",
      "['N1' 'N2' 'N3']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Read from CSV file\n",
    "data = pd.read_csv(\"test/DataSets/Tests/StringData2.csv\")\n",
    "\n",
    "#Specify the dataset's action label\n",
    "actionLabel = \"phenotype\"\n",
    "\n",
    "#Derive the attribute and phenotype array using the action label\n",
    "dataFeatures = data.drop(actionLabel,axis = 1).values\n",
    "dataActions = data[actionLabel].values\n",
    "\n",
    "#Optional: Retrieve the headers for each attribute as a length n array\n",
    "dataHeaders = data.drop(actionLabel,axis=1).columns.values\n",
    "\n",
    "print(\"Data Features\")\n",
    "print(dataFeatures)\n",
    "print(\"\\nData Actions\")\n",
    "print(dataActions)\n",
    "print(\"\\nData Headers\")\n",
    "print(dataHeaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dummy dataset does not come close to meeting the dataset requirements for training. There exist many string types in the attribute and phenotype array, as well as missing phenotype data. Aside from not meeting the requirements, the data headers and phenotype label are not too descriptive. Below StringEnumerator cleans this up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Features\n",
      "[[ 0.   1.2  1. ]\n",
      " [ 1.  -0.4  0. ]\n",
      " [ nan  0.   1. ]]\n",
      "\n",
      "Data Phenotypes\n",
      "[0. 0. 1.]\n",
      "\n",
      "Data Headers\n",
      "['Gender' 'Numeric' 'Age']\n",
      "\n",
      "Class Label\n",
      "Country\n"
     ]
    }
   ],
   "source": [
    "from skXCS import StringEnumerator\n",
    "\n",
    "#Initialize StringEnumerator object with csv filepath and class label.\n",
    "converter = StringEnumerator(\"test/DataSets/Tests/StringData2.csv\",\"phenotype\")\n",
    "\n",
    "#Change Header Names to be more descriptive using change_header_name(currentName,newName)\n",
    "converter.change_header_name(\"N1\",\"Gender\")\n",
    "converter.change_header_name(\"N2\",\"Numeric\")\n",
    "converter.change_header_name(\"N3\",\"Age\")\n",
    "\n",
    "#Change Phenotype Label to be more descriptive using change_class_name(newName)\n",
    "converter.change_class_name(\"Country\")\n",
    "\n",
    "'''\n",
    "Convert attributes to numeric data using either:\n",
    "-add_attribute_converter_random(headerName):   Given an attribute name, randomly assigns each unique attribute value an\n",
    "                                            integer value from 0 to n-1, where n = # of unique attribute values\n",
    "                                            \n",
    "-add_attribute_converter(headerName,array):   Given an attribute name, and an array of attribute values that will be\n",
    "                                            converted, converter assigns each attribute value an integer value from\n",
    "                                            0 to n-1 in the order of attribute values given in the array, where\n",
    "                                            n = length of array. This can be useful for discrete attributes where the\n",
    "                                            ordering of the attributes are important (ordinal values). For example,\n",
    "                                            an attribute may have values \"stage 1\", \"stage 2\", \"stage 3\", \"stage 4\"\n",
    "                                            to indicate stage of cancer, where the values are strings, but how they\n",
    "                                            are enumerated is crucial.\n",
    "'''\n",
    "converter.add_attribute_converter_random(\"Gender\")\n",
    "converter.add_attribute_converter(\"Age\",[\"old\",\"young\"])\n",
    "\n",
    "'''\n",
    "Convert phenotypes to numeric data using either\n",
    "-add_class_converter(array)\n",
    "-add_class_converter_random()\n",
    "Same functionality as the attribute converters\n",
    "'''\n",
    "converter.add_class_converter_random()\n",
    "\n",
    "'''\n",
    "Convert all attributes using convert_all_attributes(). Note: this is NOT a standalone method. This method\n",
    "just puts all of the changes made above \"into stone\". This method does not automatically convert anything for you. You\n",
    "MUST call this method if data transformation operations were invoked (any of the class/attributeConverter methods\n",
    "above), or you will not see any changes when you call get_params().\n",
    "'''\n",
    "converter.convert_all_attributes()\n",
    "\n",
    "#Get arrays using get_params()\n",
    "headers,classLabel,dataFeatures,dataPhenotypes = converter.get_params()\n",
    "\n",
    "print(\"Data Features\")\n",
    "print(dataFeatures)\n",
    "print(\"\\nData Phenotypes\")\n",
    "print(dataPhenotypes)\n",
    "print(\"\\nData Headers\")\n",
    "print(headers)\n",
    "print(\"\\nClass Label\")\n",
    "print(classLabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the StringEnumerator mapped all string attributes and phenotypes into numeric types, deleted the instance that was missing phenotype data, and made the data headers and class label more descriptive. Now, the data features and phenotypes are ready for training.\n",
    "\n",
    "The StringEnumerator has 2 additional methods not shown above that can also be powerful in data transformation:\n",
    "<ul>\n",
    "    <li>delete_attribute(headerName): deletes the specified attribute from all instances in the dataset</li>\n",
    "    <li>delete_all_instances_without_header_data(headerName): there may be situations where you only want to use data instances that does not have missing data for a specific attribute. This method deletes all instances who's specified attribute is missing</li>\n",
    "</ul>\n",
    "\n",
    "Even if the dataset satisfies all requirements, StringEnumerator can be used as an easy way to get data in the correct format without needing to use pandas, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Features\n",
      "[[ 1. nan  1.  4.]\n",
      " [ 2.  0.  1. nan]\n",
      " [ 4. nan  1.  2.]\n",
      " [nan  1. nan  1.]\n",
      " [ 6. nan  1.  1.]]\n",
      "\n",
      "Data Actions\n",
      "[1. 0. 1. 0. 1.]\n",
      "\n",
      "Data Headers\n",
      "['N1' 'N2' 'N3' 'N4']\n"
     ]
    }
   ],
   "source": [
    "converter = StringEnumerator(\"test/DataSets/Tests/MissingFeatureData.csv\",\"phenotype\")\n",
    "headers,actionLabel,dataFeatures,dataActions = converter.get_params()\n",
    "\n",
    "print(\"Data Features\")\n",
    "print(dataFeatures)\n",
    "print(\"\\nData Actions\")\n",
    "print(dataActions)\n",
    "print(\"\\nData Headers\")\n",
    "print(headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, StringEnumerator contains a print_invalid_attributes() method that will print out the names of any attribute with invalid typed data (including the phenotype). This can help you identify which attributes need to converted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL INVALID ATTRIBUTES & THEIR DISTINCT VALUES\n",
      "N1: male\tfemale\t\n",
      "N3: young\told\t\n",
      "phenotype (the phenotype): china\trussia\t\n"
     ]
    }
   ],
   "source": [
    "converter = StringEnumerator(\"test/DataSets/Tests/StringData2.csv\",\"phenotype\")\n",
    "converter.print_invalid_attributes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All in all, StringEnumerator makes loading data in the correct format easy. It is intuitive to use, and if any invalid operation were done with it, built in checkers will raise contextually Exceptions to help you correct the error. We will be using it to load data for the rest of this user guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Initializing scikit-XCS Estimator Object\n",
    "\n",
    "### Basic Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skXCS import XCS\n",
    "\n",
    "model = XCS()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing the above will initialize an XCS estimator object using the default hyperparameters, that is now ready for training. However, in most cases, you would want to modify some hyperparameters to your needs. We list all of the tunable hyperparameters and their descriptions:\n",
    "\n",
    "### Overview of all scikit-XCS initialization parameters and default values\n",
    "\n",
    "| Parameter Name | Requirements | Description | Default Value |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| learning_iterations | non negative integer | The number of training cycles to run | 10000 |\n",
    "| N | non negative integer | Maximum micropopulation size | 1000 |\n",
    "| p_general | float from 0 - 1 | Probability of generalizing an allele during covering | 0.5 |\n",
    "| p_crossover | float from 0 - 1 | The probability of applying crossover in an offspring classifier | 0.8 |\n",
    "| p_mutation | float from 0 - 1 | The probability of mutating one allele and the action in an offspring classifier | 0.04 |\n",
    "| p_explore | float from 0 - 1 | Probability of doing an explore cycle instead of an exploit cycle | 0.5 |\n",
    "| alpha | float | The fall of rate in the fitness evaluation | 0.1 |\n",
    "| beta | float | Learning Rate for updating statistics | 0.2 |\n",
    "| delta | float | The fraction of the mean fitness of the population below which the fitness of a classifier may be considered in its vote for deletion | 0.1 |\n",
    "| e_0 | float | The error threshold under which accuracy of a classifier can be set to 1 | 10 |\n",
    "| nu | float | Power parameter for fitness evaluation | 5 |\n",
    "| theta_del | non negative integer | Specified the threshold over which the fitness of a classifier may be considered in its deletion probability | 20 |\n",
    "| theta_GA | nonnegative float | The threshold for the GA application in an action set | 25 |\n",
    "| theta_matching | non negative integer | Number of unique actions that must be represented in the match set (otherwise, covering) | the # of unique actions in dataset|\n",
    "| theta_select | float from 0 - 1 | The fraction of the action set to be included in tournament selection | 0.5 |\n",
    "| theta_sub | non negative integer | The experience of a classifier required to be a subsumer | 20 |\n",
    "| init_e | float | The initial prediction error value when generating a new classifier (e.g in covering) | 0 |\n",
    "| init_fitness | float | The initial prediction value when generating a new classifier (e.g in covering) | 0.01 |\n",
    "| init_prediction | float | The initial prediction value when generating a new classifier (e.g in covering) | 10 |\n",
    "| do_action_set_subsumption | boolean | Do subsumption in action set | False |\n",
    "| do_GA_subsumption | boolean | Do subsumption in GA between parents and offspring | True |\n",
    "| max_payoff | float | For single step problems, what the maximum reward for correctness | 1000 |\n",
    "| fitness_reduction | float | The reduction of the fitness when generating an offspring classifier | 0.1 |\n",
    "| prediction_error_reduction | float | The reduction of the prediction error when generating an offspring classifier | 0.25 |\n",
    "\n",
    "Below, for your reference, we also put together a table that shows how the training process may be effected, if one of the above attributes were changed:\n",
    "\n",
    "| Parameter Name | Impact on training when changed |\n",
    "| :--- | :--- |\n",
    "| learning_iterations | more iterations increases accuracy (up to a point - there exists diminishing returns), but also increases training time |\n",
    "| N | for larger feature spaces, a larger N is recommended. A larger N increases training time, but too small of an N will prevent the LCS from converging on a solution |\n",
    "| p_general | the more attributes in the data set, the higher p_general should be. Too high of a p_spec and the population initialization will be less suited to detecting complex patterns, but too low can run the risk of early overfitting |\n",
    "| p_crossover | Typically between 0.5 and 1. Recommended to use default parameter |\n",
    "| p_mutation | Typically between 0.01 and 0.05. Recommended to use default parameter |\n",
    "| p_explore | Recommended to use default parameter |\n",
    "| alpha | Recommended to use default parameter |\n",
    "| beta | Typically between 0.1 and 0.2. Recommended to use default parameter |\n",
    "| delta | Recommended to use default parameter |\n",
    "| e_0 | Typically about 1 percent of max_payoff |\n",
    "| nu | recommended to be 1 for data with any level of noise. Increasing nu in clean problems improves chances of converging on optimal solution. Default of 5 is common for clean problems |\n",
    "| theta_del | Recommended to use default parameter |\n",
    "| theta_GA | Typically between 25 to 50. Recommended to use default parameter|\n",
    "| theta_matching | Recommended to use default parameter |\n",
    "| theta_select | Recommended to use default parameter |\n",
    "| theta_sub | Recommended to use default parameter |\n",
    "| init_e | Recommended to use default parameter |\n",
    "| init_fitness | Recommended to use default parameter |\n",
    "| init_prediction | Recommended to use default parameter |\n",
    "| do_action_set_subsumption | Is stronger of the two subsumptions. Subsumption is useful in problems where this is a well-defined underlying target function. |\n",
    "| do_GA_subsumption | Is weaker of the two subsumptions. Subsumption is useful in problems where this is a well-defined underlying target function. |\n",
    "| max_payoff | Arbitrary, but 1000 is commonly used |\n",
    "| fitness_reduction | Recommended to use default parameter |\n",
    "| prediction_error_reduction | Recommended to use default parameter |\n",
    "\n",
    "There also exists a few hyperparameters related to the setup and evaluation of the training process:\n",
    "\n",
    "| Parameter Name | Requirements | Description | Default Value |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| discrete_attribute_limit | non negative integer OR \"c\" OR \"d\" | Multipurpose param. If it is a nonnegative integer, discrete_attribute_limit determines the threshold that determines if an attribute will be treated as a continuous or discrete attribute. For example, if discrete_attribute_limit == 10, if an attribute has more than 10 unique values in the dataset, the attribute will be continuous. If the attribute has 10 or less unique values, it will be discrete. Alternatively, discrete_attribute_limit can take the value of \"c\" or \"d\". See next param for this. | 10 |\n",
    "| specified_attributes | numpy.ndarray of nonnegative integers of attribute indices | If discrete_attribute_limit == \"c\", attributes specified by index in this param will be continuous and the rest will be discrete. If \"d\", attributes specified by index in this param will be discrete and the rest will be continuous | empty numpy ndarray |\n",
    "| random_state | integer OR None | Set a constant random seed value to some integer (in order to obtain reproducible results). Put None if none (pseudo-random algorithm runs) | None |\n",
    "| reboot_filename | str or None | File name of pickled model to be rebooted. None by default (no model to reboot) | None |\n",
    "\n",
    "\n",
    "These hyperparameters can be set during initialization. There exists built in parameter checking to ensure each specified parameter is valid. Below is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XCS(learning_iterations = 5000,nu = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Model Training and Testing\n",
    "\n",
    "### model.fit(X, y) and how it works\n",
    "Once the model is initialized with the hyperparameters you want, you can train the model by calling the fit(X,y) method, where X is the numpy.ndarray of attributes and y is the numpy.ndarray of phenotypes. Be sure both X and y satisfy the requirements specified in Part 1, or the parameter checker will raise an Exception. Below we demo the entire process of importing relevant packages, loading data, model initialization, and model training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "match() missing 1 required positional argument: 'xcs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-d42dc9e66079>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#Initialize and train model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mXCS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_iterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mtrainedModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataFeatures\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataActions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mtrainedModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\source\\repos\\Forschungsmodul-OC\\scikit-XCS\\skXCS\\XCS.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    356\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewInstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 358\u001b[1;33m         \u001b[1;31m#self.calcInverseVariances()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveFinalMetrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\source\\repos\\Forschungsmodul-OC\\scikit-XCS\\skXCS\\XCS.py\u001b[0m in \u001b[0;36mcalcInverseVariances\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    402\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcalcInverseVariances\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpopSet\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m             \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalcInverseVariance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\source\\repos\\Forschungsmodul-OC\\scikit-XCS\\skXCS\\Classifier.py\u001b[0m in \u001b[0;36mcalcInverseVariance\u001b[1;34m(self, xcs)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[0mxcs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresetDataRef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_i\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxcs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformatData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavedRawTrainingData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxcs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetTrainState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatchCountMixing\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m                 \u001b[0mrealAction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxcs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrentTrainPhenotype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: match() missing 1 required positional argument: 'xcs'"
     ]
    }
   ],
   "source": [
    "from skXCS import StringEnumerator\n",
    "from skXCS import XCS\n",
    "import numpy as np\n",
    "\n",
    "#Use StringEnumerator to gather data\n",
    "converter = StringEnumerator(\"test/DataSets/Real/Multiplexer11.csv\",\"class\")\n",
    "headers,actionLabel,dataFeatures,dataActions = converter.get_params()\n",
    "\n",
    "#Initialize and train model\n",
    "model = XCS(learning_iterations = 5000)\n",
    "trainedModel = model.fit(dataFeatures,dataActions)\n",
    "\n",
    "trainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainedModel.env.formatData.trainFormatted[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit function returns a trained model. Below we will provide a high level overview of the scikit-XCS training algorithm. In compiling this algorithm, we aimed to make scikit-XCS as close to a 'modern' XCS as possible.\n",
    "\n",
    "<ul>\n",
    "    <li> Before training, scikit-XCS uses the training set to discern each attribute as continuous or discrete based on the <b>discrete_attribute_limit</b> and <b>specified_attributes</b> hyperparameters. It also counts the number of unique actions and determines if the problem is multiclass or binary. The training set is shuffled, and training begins. </li>\n",
    "    <li> At the start of each iteration, an unlabelled training instance is fetched from the environment. </li>\n",
    "    <li> Using the <b>p_explore</b> hyperparameter, scikit-XCS probabilistically determines if the learning iteration should be an 'explore' or 'exploit' iteration.</li>\n",
    "    <li> A match set is created. For multiclass problems, coverage is by default triggered until all actions are represented in [M] (unless the <b>theta_matching</b> hyperparameter was changed). For binary problem coverage, it is the same, but coverage also triggers (once) if there are less than 5 classifiers in [M] </li>\n",
    "    <li> If the iteration is 'explore', a random action in the match set is chosen. If the iteration is 'exploit', the classifiers in the match set each cast a weighted vote based on prediction and fitness to select the 'best action' </li>\n",
    "    <li> The chosen action is given to the environment for execution. Since it is a single step classification problem, the environment is very simple. If the chosen action matches with the correct action for that instance, the environment returns the maximum reward. If the chosen action does not match, a reward of 0 is returned. </li>\n",
    "    <li> Based on the reward, the experience, prediction, prediction error, and fitness of the classifiers in [A] are updated. If the <b>do_action_set_subsumption</b> hyperparameter is True, subsumption is done in [A] after this. </li>\n",
    "    <li> If the iteration is 'explore', GA can occur. GA then occurs if the average number of iterations since the last GA is greater than hyperparameter <b>theta_GA</b> for classifiers in [A]. Tournament selection is used to choose two parent classifiers. Mutation and uniform crossover are then applied to create offspring classifiers. If the <b>do_GA_subsumption</b> hyperparameter is True, subsumption is done between the offspring and the parents before the offspring are added back into [P]. </li>\n",
    "    <li> If the micropopulation (sum of numerosities) is greater than hyperparameter <b>N</b>, deletion occurs until it is. </li>\n",
    "    <li> Training ends when the number of iterations equals the hyperparameter <b>learning_iterations</b>.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model.predict(X), model.predict_proba(X)\n",
    "\n",
    "Once the model has finished training, you can call the predict(X) method to return an numpy.ndarray of action predictions given an array of unlabelled instances. The predict method works by using the final rule population of the model to vote on the most probable action based on classifier fitness and numerosity. Before making a prediction, parameter checkers again ensure X is a valid input (fully numeric numpy.ndarray). We refer back to the trained model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainedModel.predict(dataFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also call the predict_proba(X) method to return an nxm numpy.ndarray of phenotype probabilities, where n = number of data instances and m = number of distinct discrete phenotypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainedModel.predict_proba(dataFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation and Scoring\n",
    "You can score a set of predictions using the score(X,y) method, where X is the unlabelled test data and y is associated list of correct action values. By default, score(X,y) uses balanced accuracy to score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainedModel.score(dataFeatures,dataActions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use different scoring metrics to score the model. We will do this with a quick cross validation on the model. Below we will run a 3 fold CV on the trained model from above. Again, by default, balanced accuracy is used to score each partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "#Shuffle Data (useful for cross validation)\n",
    "formatted = np.insert(dataFeatures,dataFeatures.shape[1],dataActions,1)\n",
    "np.random.shuffle(formatted)\n",
    "dataFeatures = np.delete(formatted,-1,axis=1)\n",
    "dataActions = formatted[:,-1]\n",
    "\n",
    "np.mean(cross_val_score(trainedModel,dataFeatures,dataActions,cv=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you prefer to use a scoring method other than balanced accuracy during CV (e.g. accuracy_score, precision_score, recall_score, f1_score), that can be specified as well via the standard scoring parameter. Be aware that some of these scoring methods only accept binary endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(cross_val_score(trainedModel,dataFeatures,dataActions,cv=3,scoring=\"recall\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we demonstrate a scoring method that calls predict_prob(X):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(cross_val_score(trainedModel,dataFeatures,dataActions,cv=3,scoring=\"roc_auc\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC/PRC curves\n",
    "\n",
    "As shown above, you can compute the AUC score of the ROC using the standard scikit scoring methods/metrics. Below, we show a means to graph the ROC and PRC, and compute the AUC for both curves (on training data). Again, be aware that ROC/PRC analysis only accepts binary endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "probs = trainedModel.predict_proba(dataFeatures)\n",
    "fpr, tpr, thresholds = roc_curve(dataActions, probs[:, 1])\n",
    "prec, recall, thresholds = precision_recall_curve(dataActions, probs[:, 1])\n",
    "\n",
    "plt.plot(fpr,tpr,label=\"ROC\")\n",
    "plt.plot(recall,prec,label=\"PRC\")\n",
    "\n",
    "plt.xlabel('False Positive Rate/Recall')\n",
    "plt.ylabel('True Positive Rate/Precision')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"PRC AUC:\" + str(auc(recall, prec)))\n",
    "print(\"ROC AUC:\" + str(auc(fpr, tpr)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxilliary scikit-XCS methods\n",
    "In addition to the standard sklearn methods you can use to access training metrics, scikit-XCS provides a few of its own.\n",
    "\n",
    "| Method Name | Description |\n",
    "| :--- | :--- |\n",
    "| model.get_final_training_accuracy() | Evaluates (in real time) the final training accuracy of the model |\n",
    "| model.get_final_instance_coverage() | Evaluates (in real time) the final instance coverage of the model |\n",
    "| model.get_final_attribute_specificity_list() | Returns a list of attribute specificities (which attributes are specified the most in the rule population) |\n",
    "| model.get_final_attribute_accuracy_list() | Returns a list of attribute specificities weighted by accuracy |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final Training Accuracy: \"+str(trainedModel.get_final_training_accuracy()))\n",
    "print(\"Final Instance Coverage: \"+str(trainedModel.get_final_instance_coverage()))\n",
    "print(\"Final Attribute Specificity List: \"+str(trainedModel.get_final_attribute_specificity_list()))\n",
    "print(\"Final Attribute Accuracy List: \"+str(trainedModel.get_final_attribute_accuracy_list()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Data Collection and Export\n",
    "\n",
    "### Exporting and Accessing Iteration Tracking Data\n",
    "During training, scikit-XCS tracks a series of training statistics that may be of use. Aside from average rule population generality (which is tracked at least once an epoch), these following statistics are tracked every iteration:\n",
    "<ul>\n",
    "    <li>Approximate Training Accuracy (50 pt moving average)</li>\n",
    "    <li>Macropopulation size</li>\n",
    "    <li>Micropopulation size</li>\n",
    "    <li>Match set size</li>\n",
    "    <li>Action set size</li>\n",
    "    <li>Average iteration age of classifiers in action set</li>\n",
    "    <li>Number of classifiers subsumed in iteration</li>\n",
    "    <li>Number of crossover operations performed in iteration (GA operation)</li>\n",
    "    <li>Number of mutation operations performed in iteration (GA operation)</li>\n",
    "    <li>Number of covering operations performed in iteration</li>\n",
    "    <li>Number of macroclassifiers deleted in iteration</li>\n",
    "    <li>Total training time</li>\n",
    "    <li>Total matching time</li>\n",
    "    <li>Total deletion time</li>\n",
    "    <li>Total subsumption time</li>\n",
    "    <li>Total selection time</li>\n",
    "    <li>Total evaluation time</li>\n",
    "</ul>\n",
    "These statistics are not time intensive to compute, and thus can be tracked every iteration. \n",
    "\n",
    "Why is collecting this kind of iteration by iteration data important? Unlike many other machine learning models, as the XCS trains, it is easy to visualize its training process, such as how its accuracy or its macropopulation size changes over time. Thus, doing iteration tracking will allow you to export this training progess data (as you will see below) to aid in this visualization. Also, LCSs are stochastic algorithms with no clear stop criteria, so tracking these metrics will allow you to identify when training has stagnated, which can help in future hyperparameter optimization.\n",
    "\n",
    "To access this iteration tracking data, you can call the following method that exports the entire tracking data record from training into a local folder:\n",
    "\n",
    "| Method Name | Description |\n",
    "| :--- | :--- |\n",
    "| model.export_iteration_tracking_data(filename) | Exports all tracked data from training |\n",
    "\n",
    "The filename paramater is optional. By default, the program saves a CSV file named **iterationData.csv** into your local directory. However, you can set the filename param as an absolute path with the format **/filepath/filename.csv** which will save the data into any specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainedModel.export_iteration_tracking_data(\"defaultExportDir/iterationData.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exported CSV is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterationData = pd.read_csv(\"defaultExportDir/iterationData.csv\")\n",
    "display(iterationData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is exported to a CSV, you can plot trendlines for each statistic. We will use pandas and matplotlib as a demo below. Please note that for the average accuracy and average generality graphs, the first number of iterations is not as \"accurate\" as the rest of the iterations. The average accuracy moving average only begins moving at iteration 50, and average generality is not tracked until a few hundred iterations in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def cumulativeFreq(freq):\n",
    "    a = []\n",
    "    c = []\n",
    "    for i in freq:\n",
    "        a.append(i+sum(c))\n",
    "        c.append(i)\n",
    "    return np.array(a)\n",
    "\n",
    "def movingAvg(a,threshold=300):\n",
    "    weights = np.repeat(1.0,threshold)/threshold\n",
    "    conv = np.convolve(a,weights,'valid')\n",
    "    return np.append(conv,np.full(threshold-1,conv[conv.size-1]),)\n",
    "\n",
    "dataTracking = pd.read_csv(\"defaultExportDir/iterationData.csv\")\n",
    "\n",
    "iterations = dataTracking[\"Iteration\"].values\n",
    "accuracy = dataTracking['Accuracy (approx)'].values\n",
    "generality = dataTracking['Average Population Generality'].values\n",
    "macroPop = dataTracking[\"Macropopulation Size\"].values\n",
    "microPop = dataTracking[\"Micropopulation Size\"].values\n",
    "mSize = dataTracking[\"Match Set Size\"].values\n",
    "aSize = dataTracking[\"Action Set Size\"].values\n",
    "experience = dataTracking[\"Average Iteration Age of Action Set Classifiers\"].values\n",
    "subsumption = dataTracking[\"# Classifiers Subsumed in Iteration\"].values\n",
    "crossover = dataTracking[\"# Crossover Operations Performed in Iteration\"].values\n",
    "mutation = dataTracking[\"# Mutation Operations Performed in Iteration\"].values\n",
    "covering = dataTracking[\"# Covering Operations Performed in Iteration\"].values\n",
    "deletion = dataTracking[\"# Deletion Operations Performed in Iteration\"].values\n",
    "\n",
    "gTime = dataTracking[\"Total Global Time\"].values\n",
    "mTime = dataTracking[\"Total Matching Time\"].values\n",
    "delTime = dataTracking[\"Total Deletion Time\"].values\n",
    "subTime = dataTracking[\"Total Subsumption Time\"].values\n",
    "selTime = dataTracking[\"Total GA Time\"].values\n",
    "evalTime = dataTracking[\"Total Evaluation Time\"].values\n",
    "\n",
    "plt.plot(iterations,accuracy,label=\"approx accuracy\")\n",
    "plt.plot(iterations,generality,label=\"avg generality\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('accuracy/generality')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(iterations,macroPop,label=\"macroPop Size\")\n",
    "plt.plot(iterations,microPop,label=\"microPop Size\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Macro/MicroPop Size')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(iterations,mSize,label=\"[M] size\")\n",
    "plt.plot(iterations,movingAvg(mSize),label=\"[M] size movingAvg\")\n",
    "plt.plot(iterations,aSize,label=\"[A] size\")\n",
    "plt.plot(iterations,movingAvg(aSize),label=\"[A] size movingAvg\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('[M]/[A] size per iteration')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(iterations,experience)\n",
    "plt.ylabel('Average [A] Classifier Age')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(iterations,cumulativeFreq(subsumption),label=\"Subsumption Count\")\n",
    "plt.plot(iterations,cumulativeFreq(crossover),label=\"Crossover Count\")\n",
    "plt.plot(iterations,cumulativeFreq(mutation),label=\"Mutation Count\")\n",
    "plt.plot(iterations,cumulativeFreq(deletion),label=\"Deletion Count\")\n",
    "plt.plot(iterations,cumulativeFreq(covering),label=\"Covering Count\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cumulative Operations Count Over Iterations')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(iterations,mTime,label=\"Matching Time\")\n",
    "plt.plot(iterations,delTime+mTime,label=\"Deletion Time\")\n",
    "plt.plot(iterations,subTime+delTime+mTime,label=\"Subsumption Time\")\n",
    "plt.plot(iterations,selTime+subTime+delTime+mTime,label=\"GA Time\")\n",
    "plt.plot(iterations,evalTime+selTime+subTime+delTime+mTime,label=\"Evaluation Time\")\n",
    "plt.plot(iterations,gTime,label=\"Total Time\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cumulative Time (Stacked)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting and Accessing Rule Population Data\n",
    "Post-training, you also have the option to export the final rule population. This can be done be calling one of the following methods:\n",
    "\n",
    "| Method Name | Description |\n",
    "| :--- | :--- |\n",
    "| model.export_final_rule_population(filename,headerNames,classLabel) | Exports the entire final rule population with traditional rule representation|\n",
    "| model.export_final_rule_population_DCAL(filename,headerNames,classLabel) | Exports the entire final rule population with DCAL rule representation|\n",
    "\n",
    "The headerNames and classLabel params are optional. If they are not provided, default header and class names will be populated. But they make the exported csv more descriptive.\n",
    "\n",
    "The filename param is also optional. By default, the program saves a CSV file named **populationData.csv** into your local directory. However, you can set the filename param as an absolute path with the format **/filepath/filename.csv** which will save the data into any specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainedModel.export_final_rule_population(\"defaultExportDir/fileRulePopulation.csv\",headers,classLabel)\n",
    "\n",
    "populationData2 = pd.read_csv(\"defaultExportDir/fileRulePopulation.csv\")\n",
    "display(populationData2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the above final rule population exported in DCAL format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainedModel.export_final_rule_population_DCAL(\"defaultExportDir/popData2.csv\",headers,classLabel)\n",
    "\n",
    "populationData2 = pd.read_csv(\"defaultExportDir/popData2.csv\")\n",
    "display(populationData2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Population Reboot\n",
    "The final feature to introduce is the ability to \"reboot\" past rule populations. In some cases, you might want to run training, pause training, and resume training at some point in the future. Alternatively, you might want to train the generated rule population on new datasets or with different hyperparameters. The population reboot feature allows you to \"save\" the current rule population into a txt file for future use. You can later initialize a new XCS estimator with this txt file. When you run fit, the new XCS estimator will effectively start off where the saved model left off, beginning with the saved rule population, as well as starting at the next sequential learning iteration and adding to the last model's training times. A demo of this is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = StringEnumerator(\"test/DataSets/Real/Multiplexer11.csv\",\"class\")\n",
    "headers,classLabel,dataFeatures,dataActions = converter.get_params()\n",
    "\n",
    "#Run 1\n",
    "model = XCS(learning_iterations=1000)\n",
    "model.fit(dataFeatures,dataActions)\n",
    "print(model.score(dataFeatures,dataActions))\n",
    "model.pickle_model(\"defaultExportDir/savedModel1\")\n",
    "\n",
    "#Run 2 w/ rebooted first model\n",
    "model2 = XCS(learning_iterations=1000,reboot_filename=\"defaultExportDir/savedModel1\")\n",
    "print(model2.score(dataFeatures,dataActions))\n",
    "model2.fit(dataFeatures,dataActions)\n",
    "print(model2.score(dataFeatures,dataActions))\n",
    "model2.pickle_model(\"defaultExportDir/savedModel2\")\n",
    "\n",
    "#Run 3 w/ rebooted second model\n",
    "model3 = XCS(learning_iterations=1000,reboot_filename=\"defaultExportDir/savedModel2\")\n",
    "print(model3.score(dataFeatures,dataActions))\n",
    "model3.fit(dataFeatures,dataActions)\n",
    "print(model3.score(dataFeatures,dataActions))\n",
    "model3.pickle_model(\"defaultExportDir/savedModel3\")\n",
    "\n",
    "#Run 4 w/ rebooted third model\n",
    "model4 = XCS(learning_iterations=1000,reboot_filename=\"defaultExportDir/savedModel3\")\n",
    "print(model4.score(dataFeatures,dataActions))\n",
    "model4.fit(dataFeatures,dataActions)\n",
    "print(model4.score(dataFeatures,dataActions))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
