\documentclass{ocsmnar}



% Adjust this to the language used.
\usepackage[ngerman]{babel}
\usepackage{hyperref}
\usepackage{amsmath}

\begin{document}


\title{Implementierung und Evaluation von Mixing-Modell-Alternativen für XCS}


\author{Markus Weis}

\maketitle




\section{Einleitung}
In modernen Michigan-Style Learning Classifier Systemen (LCS) wird üblicherweise aus einzelnen Classifier Vorhersagen eine Gesamtvorhersage berechnet. In XCS wird die Prediction über eine Fitness gewichtete Summe der einzelnen Predictions berechnet. Diese Fitness ist ein heuristisch Parameter, der inkrementell berechnet wird. Wilson schreibt, dass es wohl verschieden Möglichkeiten gibt, die Gesamtvorhersage zu berechnen, behält aber dieses Modell ohne weitere Ausführung in seinen späteren Arbeiten bei \cite{Wilson95}. 
Drugowitsch setzt in seinem Buch "Design and Analysis of Learning Classifier Systems - A Probabilistic Approach" \cite{book} dort an. Er beschreibt verschiedene Mixing Modelle und wertet diese empirisch aus. 
Einen dieser Modelle wird in der vorliegenden Arbeit beschrieben und eine Implementierung versucht: Inverse Variance Mixing. Aus den heuristisch Modellen schnitt dieses stets am besten ab, während die Laufzeit der  analytischen Lösung nicht angemessen ist. Das Modell wird im Open-Source Projekt scikit-XCS \cite{repo}, einer scikit-learn Erweiterung für XCS, implementiert. 

\section{Inverse Variance Mixing}
Die Definition des Inverse Variance Mixing ist \cite{book} entnommen. 

$$ \tau_{k}^{-1} = (c_{k} - D^{X}) \sum_{n}^{N} m(x_{n}) (\hat{w}_{k}^T x_{n} - y_{n})^{2} $$


\section{Probleme}
Wie bereits beschrieben handelt es sich bei der eigentlichen Definition um Regression. Bei Regression ist es unwahrscheinlich bzw. unmöglich, je nach Modell, auf einen perfekten Classifier zu treffen. Das Problem bei einem perfekten Classifier ist, dass der hintere Teil der Formel 0 ergibt. Das wird zum Problem, wenn man zur Berechnung der Inversen Varianz den Umkehrbruch bildet. Im Buch ist das nicht weiter definiert. In dieser Implementation habe ich in diesem Fall Unendlich (numpy.inf) eingesetzt. Dadurch bekommt man schließlich im Mixing Probleme. Zwar bekommt man bei einem Unendlich beim Mixing-Parameter $g_{k}$ bei allen anderen Classifiern, die nicht Unendlich sind 0 raus, allerdings ist $f\frac{numpy.inf}{numpy.inf}$ nicht definiert. Stattdessen wird $g_{k}$ im Falle eines Classifiers k im Matchset mit $\gamma_{k} = \infty$ wie folgt berechnet: $g_{k} = \frac{1}{c_{inf}} $ für alle Classifier k mit $\gamma_{k} = \infty$ und $g_{k} = 0$ für alle anderen Classifier. $c_{inf}$ ist die Anzahl der Classifier mit $\gamma_{k} = \infty$.

Unendlich 
Minus 
Batch Ansatz statt Iterativer Ansatz -> Laufzeit
Nicht für XCS gedacht sondern für allgemeinere LCS Systeme 
scikit trainiert die Modelle immer im ganzen. Ich konnte die Accuracy nicht Ploten lassen 

\section{Ergebnisse}

\section{Fazit}





XCS ist ein Michigan-Style Learning Classifier System 









\bibliographystyle{ACM-Reference-Format}
\bibliography{References}


\end{document}
